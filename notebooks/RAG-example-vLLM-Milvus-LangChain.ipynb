{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5c33085-589e-4838-868e-c22691d288e6",
   "metadata": {},
   "source": [
    "# RAG example with vLLM (Mistral-7b-Instruct-v0.2), Milvus and LangChain\n",
    "\n",
    "Prerequisites:\n",
    "- a vLLM inference endpoint\n",
    "- a Milvus instance with some defined collection (e.g. RHOAI documentation)\n",
    "- configured the Milvus credentials in the Workbench environment using the RHOAI dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc6755b-1e57-4a51-adde-341362b5341a",
   "metadata": {},
   "source": [
    "### Install required library depencencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2060758c-f018-4878-8cf2-298c0f5b27a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q einops==0.7.0 langchain==0.1.9 pymilvus==2.3.6 sentence-transformers==2.4.0 openai==1.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbdbb7f3-73c3-4e53-b46c-9e8fc2dd60df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb1cb8-e5c6-4b91-a491-bfa6fdcb3b50",
   "metadata": {},
   "source": [
    "### Base parameters initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e8d2161-460b-48bb-b3fa-cc9edaebb5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace values according to your Milvus deployment\n",
    "INFERENCE_SERVER_URL = \"https://test-llm10-test-llm.apps.cluster-bjzm6.bjzm6.sandbox2625.opentlc.com\"\n",
    "MODEL_NAME = \"/mnt/models/\"\n",
    "MAX_TOKENS=1024\n",
    "TOP_P=0.95\n",
    "TEMPERATURE=0.01\n",
    "PRESENCE_PENALTY=1.03\n",
    "MILVUS_HOST = \"vectordb-milvus.milvus.svc.cluster.local\"\n",
    "MILVUS_PORT = 19530\n",
    "MILVUS_USERNAME = os.getenv('MILVUS_USERNAME')\n",
    "MILVUS_PASSWORD = os.getenv('MILVUS_PASSWORD')\n",
    "MILVUS_COLLECTION = \"demo_collection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5ea25a-534c-422a-9bd6-db35230b20de",
   "metadata": {},
   "source": [
    "### You need the SSL cert if you have self-signed certs for your LLM inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec739188-21a6-4f49-82b6-04ab71cd320b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "import OpenSSL\n",
    "import socket\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def save_srv_cert(host, port=443) :\n",
    "    dst = (host, port)\n",
    "    sock = socket.create_connection(dst)\n",
    "    context = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n",
    "    connection = OpenSSL.SSL.Connection(context, sock)\n",
    "    connection.set_tlsext_host_name(host.encode('utf-8'))\n",
    "    connection.set_connect_state()\n",
    "    try:\n",
    "        connection.do_handshake()\n",
    "        certificate = connection.get_peer_certificate()\n",
    "    except:\n",
    "        certificate = connection.get_peer_certificate()\n",
    "    pem_file = certificate.to_cryptography().public_bytes(serialization.Encoding.PEM)\n",
    "    cert_filename = f\"cert-{host}.cer\"\n",
    "    with open(cert_filename, \"w\") as fout:\n",
    "        fout.write(pem_file.decode('utf8'))\n",
    "    return cert_filename\n",
    "\n",
    "# Extract the hostname\"\n",
    "hostname = urlparse(INFERENCE_SERVER_URL).netloc\n",
    "os.environ[\"SSL_CERT_FILE\"] = save_srv_cert(hostname, port=443)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a0ada-954f-45d1-95ab-9f6faa460bbc",
   "metadata": {},
   "source": [
    "### Initialize the embeddings model, and the vector DB connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58ffba30-2472-4094-848f-cb860696dd61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 2.4.0.dev0, however, your version is 2.4.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "model_kwargs = {'trust_remote_code': True}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    #model_name=\"nomic-ai/nomic-embed-text-v1\",\n",
    "    model_kwargs=model_kwargs,\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"host\": MILVUS_HOST, \"port\": MILVUS_PORT, \"user\": MILVUS_USERNAME, \"password\": MILVUS_PASSWORD},\n",
    "    collection_name=MILVUS_COLLECTION,\n",
    "    metadata_field=\"metadata\",\n",
    "    text_field=\"page_content\",\n",
    "    drop_old=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dab203f-8669-4e6c-bf08-6bd13fe7038e",
   "metadata": {},
   "source": [
    "### Setup a prompt for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edb49cca-0a4d-464b-a48d-ed492a0d6dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template=\"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant named HatBot answering questions.\n",
    "You will be given a question you need to answer, and a context to provide you with information. You must answer the question based as much as possible on this context.\n",
    "Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Context: \n",
    "{context}\n",
    "\n",
    "Question: {question} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c0b9b9-83fe-4482-85d3-52252b05007d",
   "metadata": {},
   "source": [
    "### Initialize the LLM and use LangChain to connect the RAG part to the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ce210ce-d473-4549-8163-ad5f2e4003ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm =  VLLMOpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base=f\"{INFERENCE_SERVER_URL}/v1\",\n",
    "    model_name=MODEL_NAME,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    top_p=TOP_P,\n",
    "    temperature=TEMPERATURE,\n",
    "    presence_penalty=PRESENCE_PENALTY,\n",
    "    streaming=True,\n",
    "    verbose=False,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "            ),\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "        return_source_documents=True\n",
    "        )\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463829af-88d3-4ed9-b084-d44996649e74",
   "metadata": {},
   "source": [
    "### Test the RAG setup by asking a question from the indexed documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87baa917-2e9c-4769-b393-dba9130925b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a Data Science project in Red Hat OpenShift AI Self-Managed, you can follow these general steps:\n",
      "\n",
      "1. Log in to your OpenShift cluster using the OpenShift CLI or the web console.\n",
      "2. Create a new project by running the following command in the OpenShift CLI: `oc new-project <project_name>`. Replace `<project_name>` with the desired name for your project.\n",
      "3. Once the project is created, you can create a new container image based on one of the provided images such as PyTorch, Minimal Python, TrustyAI, or HabanaAI. You can use the `oc create` command to create a new image stream and build the image. For example, to create an image stream named `my-pytorch-image` based on the PyTorch image, run `oc create is my-pytorch-image --image=registry.redhat.io/rhel7/python-36-pytorch`.\n",
      "4. After the image is built, you can create a new container based on the image stream and start a new instance of the Jupyter Notebook server. For example, to create a new container named `my-notebook` based on the `my-pytorch-image` image stream, run `oc create dc my-notebook --image-stream my-pytorch-image --container-port 8888`.\n",
      "5. Once the container is running, you can access the Jupyter Notebook server by opening a web browser and navigating to the URL generated by OpenShift. The URL will be in the format `https://<your_openshift_cluster_url>/api/v1/namespaces/<project_name>/services/<notebook_service_name>:8888`. Replace `<your_openshift_cluster_url>` with the URL of your OpenShift cluster, `<project_name>` with the name of your project, and `<notebook_service_name>` with the name of the service associated with your notebook container.\n",
      "\n",
      "These are the general steps to create a Data Science project in Red Hat OpenShift AI Self-Managed. However, keep in mind that the specific steps may vary depending on your use case and the tools and frameworks you plan to use."
     ]
    }
   ],
   "source": [
    "question = \"How can I create a Data Science Project?\"\n",
    "result = qa_chain.invoke({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f420adf7-6283-44f5-825f-8ce0b55c77ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.6/html-single/getting_started_with_red_hat_openshift_ai_self-managed/index\n",
      "https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.6/html-single/serving_models/index\n",
      "https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.6/html-single/working_on_data_science_projects/index\n",
      "https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.6/html-single/release_notes/index\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicates(input_list):\n",
    "    unique_list = []\n",
    "    for item in input_list:\n",
    "        if item.metadata['source'] not in unique_list:\n",
    "            unique_list.append(item.metadata['source'])\n",
    "    return unique_list\n",
    "\n",
    "results = remove_duplicates(result['source_documents'])\n",
    "\n",
    "for s in results:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d186466b-4ce6-426e-9989-442403c3d35e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
